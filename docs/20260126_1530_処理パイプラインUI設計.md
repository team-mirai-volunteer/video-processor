# 処理パイプラインUI設計

## 目的

**ユーザーが、動画処理の各ステップ（キャッシュ→音声抽出→文字起こし→AI校正）を個別に実行・確認できず、全体を一括実行するしかない状態を解消するため。**

現状は「文字起こし作成」ボタンで4ステップ一括実行のみ。これを縦向きアコーディオン形式のUIに変更し、各ステップの個別実行と進捗確認を可能にする。

---

## 現状整理

### 処理パイプライン構成

| Step | UseCase | 処理内容 | 依存条件 |
|------|---------|---------|---------|
| 1 | `CacheVideoUseCase` | Drive → GCS ストリーム転送 | なし |
| 2 | `ExtractAudioUseCase` | GCS動画 → 音声抽出 (FLAC) | Step 1完了（gcsUri存在） |
| 3 | `TranscribeAudioUseCase` | 音声 → Speech-to-Text | Step 2完了（audioBuffer） |
| 4 | `RefineTranscriptUseCase` | 文字起こし → AI校正 | Step 3完了（Transcription存在） |

### 現状のAPI

- `POST /api/videos/:videoId/transcribe` - 全ステップ一括実行
- `POST /api/videos/:videoId/refine-transcript` - Step 4のみ個別実行

### 現状のUI

- 「文字起こし作成」ボタン → 全4ステップ一括
- 「AIで校正する」ボタン → Step 4のみ

---

## 設計概要

### 新UI: 縦向きアコーディオン形式

```
┌─────────────────────────────────────────────────────────────────────────────┐
│  処理パイプライン                              [▶ 全ステップ実行]             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ▼ ① キャッシュ (Drive → GCS)                              ✓ 完了          │
│  ├─ GCS URI: gs://bucket/video-xxx                                          │
│  ├─ 有効期限: 2026-01-27 15:00                                              │
│  └─ [再実行]                                                                 │
│                                                                             │
│  ▼ ② 音声抽出                                              ✓ 完了          │
│  ├─ フォーマット: FLAC                                                       │
│  └─ [再実行]                                                                 │
│                                                                             │
│  ▼ ③ 文字起こし                                            ● 実行中...     │
│  ├─ Speech-to-Text APIで処理中...                                           │
│  └─                                                                          │
│                                                                             │
│  ▷ ④ AI校正                                                ○ 待機中         │
│     (③が完了すると実行可能になります)                      [実行]          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## タスク分割

以下の3タスクは並列実行可能。

---

# タスク1: Cloud Run バックエンド

## 概要

4つの個別エンドポイントを追加し、各ステップを独立して実行可能にする。

## 新規APIエンドポイント

### 1. POST /api/videos/:videoId/cache

動画をGoogle DriveからGCSにキャッシュする。

**リクエスト**: なし

**レスポンス** (`CacheVideoResponse`):
```typescript
interface CacheVideoResponse {
  videoId: string;
  gcsUri: string;
  expiresAt: string; // ISO 8601
  cached: boolean;   // true=既存キャッシュ利用, false=新規キャッシュ
}
```

**HTTP Status**: 200 OK

**エラー**:
- 404: Video not found

### 2. POST /api/videos/:videoId/extract-audio

GCSにキャッシュされた動画から音声を抽出する。

**リクエスト**: なし

**レスポンス** (`ExtractAudioResponse`):
```typescript
interface ExtractAudioResponse {
  videoId: string;
  format: 'flac';
  sizeBytes: number;
}
```

**HTTP Status**: 200 OK

**エラー**:
- 404: Video not found
- 400: Video is not cached in GCS (gcsUri is null)

**注意**: このエンドポイントは音声をメモリ上で処理し、結果は返さない（次のステップで使用するため、このエンドポイント単体では意味がない）。実際には `TranscribeAudioUseCase` が内部で `ExtractAudioUseCase` を呼ぶため、このエンドポイントは「音声抽出のみ実行して結果を確認したい」デバッグ用途。実装上は音声バッファを返すのではなく、処理完了のステータスを返す。

### 3. POST /api/videos/:videoId/transcribe-audio

音声から文字起こしを実行する。Step 1, 2が完了していない場合は自動的に実行する。

**リクエスト**: なし

**レスポンス** (`TranscribeAudioResponse`):
```typescript
interface TranscribeAudioResponse {
  videoId: string;
  transcriptionId: string;
  segmentsCount: number;
  durationSeconds: number;
}
```

**HTTP Status**: 200 OK

**エラー**:
- 404: Video not found

### 4. POST /api/videos/:videoId/refine-transcript

既存エンドポイント。変更なし。

## 変更ファイル

| ファイル | 変更内容 |
|---------|---------|
| `apps/backend/src/presentation/routes/videos.ts` | 3つの新規エンドポイント追加 |
| `apps/shared/types/api.ts` | 新規レスポンス型定義追加 |

## 実装詳細

### videos.ts への追加

```typescript
// POST /api/videos/:videoId/cache
router.post('/:videoId/cache', async (req, res, next) => {
  // cacheVideoUseCase.execute(videoId) を呼び出し
});

// POST /api/videos/:videoId/extract-audio
router.post('/:videoId/extract-audio', async (req, res, next) => {
  // extractAudioUseCase.execute(videoId, 'flac') を呼び出し
  // audioBufferは返さず、sizeのみ返す
});

// POST /api/videos/:videoId/transcribe-audio
router.post('/:videoId/transcribe-audio', async (req, res, next) => {
  // 1. cacheVideoUseCase.execute(videoId)
  // 2. extractAudioUseCase.execute(videoId, 'flac')
  // 3. transcribeAudioUseCase.execute({ videoId, audioBuffer, mimeType })
});
```

### 既存エンドポイントの扱い

`POST /api/videos/:videoId/transcribe` は後方互換性のため維持する。内部で `CreateTranscriptUseCase` を呼び出す動作は変更なし。

---

# タスク2: Next.js バックエンド（Server Actions）

## 概要

Cloud Runバックエンドの新規APIを呼び出すServer Actionsを追加する。

## 新規ファイル

### apps/webapp/src/server/presentation/actions/cacheVideo.ts

```typescript
'use server';

import type { CacheVideoResponse } from '@video-processor/shared';
import { getBackendClient } from '../../infrastructure/clients/get-backend-client';

export async function cacheVideo(videoId: string): Promise<CacheVideoResponse> {
  return getBackendClient().cacheVideo(videoId);
}
```

### apps/webapp/src/server/presentation/actions/extractAudio.ts

```typescript
'use server';

import type { ExtractAudioResponse } from '@video-processor/shared';
import { getBackendClient } from '../../infrastructure/clients/get-backend-client';

export async function extractAudio(videoId: string): Promise<ExtractAudioResponse> {
  return getBackendClient().extractAudio(videoId);
}
```

### apps/webapp/src/server/presentation/actions/transcribeAudio.ts

```typescript
'use server';

import type { TranscribeAudioResponse } from '@video-processor/shared';
import { getBackendClient } from '../../infrastructure/clients/get-backend-client';

export async function transcribeAudio(videoId: string): Promise<TranscribeAudioResponse> {
  return getBackendClient().transcribeAudio(videoId);
}
```

## 変更ファイル

| ファイル | 変更内容 |
|---------|---------|
| `apps/webapp/src/server/infrastructure/clients/backend-client.ts` | 3つの新規メソッド追加 |
| `apps/webapp/src/server/infrastructure/clients/mock-backend-client.ts` | モック追加（必要に応じて） |

## backend-client.ts への追加

```typescript
async cacheVideo(videoId: string): Promise<CacheVideoResponse> {
  return fetchBackend<CacheVideoResponse>(`/api/videos/${videoId}/cache`, {
    method: 'POST',
  });
}

async extractAudio(videoId: string): Promise<ExtractAudioResponse> {
  return fetchBackend<ExtractAudioResponse>(`/api/videos/${videoId}/extract-audio`, {
    method: 'POST',
  });
}

async transcribeAudio(videoId: string): Promise<TranscribeAudioResponse> {
  return fetchBackend<TranscribeAudioResponse>(`/api/videos/${videoId}/transcribe-audio`, {
    method: 'POST',
  });
}
```

---

# タスク3: フロントエンド（React コンポーネント）

## 概要

縦向きアコーディオン形式の処理パイプラインUIコンポーネントを作成し、video-detail-client.tsx に統合する。

## 新規コンポーネント

### apps/webapp/src/components/features/processing-pipeline/index.tsx

エクスポート用のバレルファイル。

### apps/webapp/src/components/features/processing-pipeline/processing-pipeline.tsx

メインコンポーネント。

**Props**:
```typescript
interface ProcessingPipelineProps {
  video: VideoWithRelations;
  transcription: GetTranscriptionResponse | null;
  refinedTranscription: GetRefinedTranscriptionResponse | null;
  onStepComplete: () => void; // ステップ完了時にデータ再取得をトリガー
}
```

**内部状態**:
```typescript
interface StepState {
  status: 'pending' | 'ready' | 'running' | 'completed' | 'error';
  result?: unknown;
  error?: string;
}

// 各ステップの状態
const [steps, setSteps] = useState<{
  cache: StepState;
  extractAudio: StepState;
  transcribe: StepState;
  refine: StepState;
}>
```

**ステップ状態の初期化ロジック**:
```typescript
// video.gcsUri が存在 → cache: completed
// video.transcription が存在 → extractAudio: completed, transcribe: completed
// video.refinedTranscription が存在 → refine: completed
```

### apps/webapp/src/components/features/processing-pipeline/pipeline-step.tsx

個別ステップのアコーディオンアイテム。

**Props**:
```typescript
interface PipelineStepProps {
  stepNumber: number;
  title: string;
  description: string;
  status: 'pending' | 'ready' | 'running' | 'completed' | 'error';
  isExpanded: boolean;
  onToggle: () => void;
  onExecute: () => void;
  canExecute: boolean;
  children?: React.ReactNode; // ステップ詳細情報
  error?: string;
}
```

**UI仕様**:
- ヘッダー: ステップ番号、タイトル、ステータスバッジ、実行ボタン
- 展開時: 詳細情報（結果やエラーメッセージ）
- ステータスアイコン:
  - pending: グレーの○
  - ready: 青の○
  - running: スピナー
  - completed: 緑のチェックマーク
  - error: 赤の×

## 変更ファイル

| ファイル | 変更内容 |
|---------|---------|
| `apps/webapp/src/app/videos/[id]/video-detail-client.tsx` | ProcessingPipelineコンポーネントを統合 |

## video-detail-client.tsx の変更

現在の「文字起こしカード」を `ProcessingPipeline` コンポーネントに置き換える。

**Before**:
```tsx
<Card>
  <CardHeader>
    <CardTitle>文字起こし</CardTitle>
  </CardHeader>
  <CardContent>
    {/* 現在の文字起こしUI */}
  </CardContent>
</Card>
```

**After**:
```tsx
<ProcessingPipeline
  video={video}
  transcription={transcription}
  refinedTranscription={refinedTranscription}
  onStepComplete={pollStatus}
/>
```

## UI詳細仕様

### 各ステップの表示内容

#### Step 1: キャッシュ
- タイトル: 「キャッシュ (Drive → GCS)」
- 詳細表示:
  - GCS URI
  - 有効期限
  - キャッシュ済み / 新規キャッシュ

#### Step 2: 音声抽出
- タイトル: 「音声抽出」
- 詳細表示:
  - フォーマット: FLAC
  - サイズ（バイト数）

#### Step 3: 文字起こし
- タイトル: 「文字起こし」
- 詳細表示:
  - セグメント数
  - 音声長（秒）

#### Step 4: AI校正
- タイトル: 「AI校正」
- 詳細表示:
  - 文数
  - 辞書バージョン

### 実行可能条件

| ステップ | 実行可能条件 |
|---------|-------------|
| Step 1 | 常に実行可能 |
| Step 2 | Step 1が完了済み（video.gcsUriが存在） |
| Step 3 | Step 2が完了済み（※実際は内部でStep 1,2を自動実行） |
| Step 4 | Step 3が完了済み（transcriptionが存在） |

### 「全ステップ実行」ボタン

既存の `transcribeVideo` Server Action を呼び出す。従来の動作を維持。

---

## 共通: 型定義追加

### apps/shared/types/api.ts への追加

```typescript
// ============================================================================
// Pipeline Step APIs
// ============================================================================

/**
 * POST /api/videos/:videoId/cache response
 */
export interface CacheVideoResponse {
  videoId: string;
  gcsUri: string;
  expiresAt: string; // ISO 8601
  cached: boolean;
}

/**
 * POST /api/videos/:videoId/extract-audio response
 */
export interface ExtractAudioResponse {
  videoId: string;
  format: 'flac';
  sizeBytes: number;
}

/**
 * POST /api/videos/:videoId/transcribe-audio response
 */
export interface TranscribeAudioResponse {
  videoId: string;
  transcriptionId: string;
  segmentsCount: number;
  durationSeconds: number;
}
```

---

## 依存関係

```
┌─────────────────────────────────────────────────────────────┐
│                     apps/shared                             │
│  (型定義: CacheVideoResponse, ExtractAudioResponse, etc.)   │
└─────────────────────────────────────────────────────────────┘
                    ↑                    ↑
                    │                    │
┌───────────────────┴──────┐  ┌─────────┴───────────────────┐
│   タスク1: Cloud Run      │  │  タスク2: Next.js Backend    │
│   バックエンド            │  │  (Server Actions)            │
│   ・新規エンドポイント    │  │  ・backend-client追加        │
│   ・videos.ts             │  │  ・actions追加               │
└──────────────────────────┘  └───────────────────────────────┘
                                          ↑
                                          │
                              ┌───────────┴───────────────┐
                              │  タスク3: フロントエンド   │
                              │  ・ProcessingPipeline     │
                              │  ・video-detail-client    │
                              └───────────────────────────┘
```

**並列実行の前提**: `apps/shared` の型定義は最初に追加する（タスク1の一部として実施、または事前作業として切り出す）。

---

## テスト方針

### タスク1: Cloud Run バックエンド
- ユニットテスト: 各エンドポイントのハンドラー
- 統合テスト: UseCase呼び出しと実際のDB操作

### タスク2: Next.js バックエンド
- ユニットテスト: Server Actionsの呼び出し確認（モック使用）

### タスク3: フロントエンド
- E2Eテスト: パイプラインUIの操作（Playwright）

---

## 後方互換性

- 既存の `POST /api/videos/:videoId/transcribe` エンドポイントは維持
- 既存の `POST /api/videos/:videoId/refine-transcript` エンドポイントは維持
- UIの「全ステップ実行」ボタンは既存APIを使用
